{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supreme-hurricane",
   "metadata": {},
   "source": [
    "#### Submitted By: Lior Kricheli, Eliran Malka and Itamar Yacoby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hollywood-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap \n",
    "import seaborn as sns\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-lucas",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Model</th>\n",
    "    <th>#parameters</th>\n",
    "    <th>epochs</th>\n",
    "    <th>train accuracy</th>\n",
    "    <th>test accuracy</th>\n",
    "  </tr>\n",
    "    \n",
    "  <!-- copy this block once for every model you tested -->  \n",
    "  <tr> \n",
    "    <td>...</td>   <!-- Model -->\n",
    "    <td>>... </td> <!-- Parameters -->\n",
    "    <td>>...</td> <!-- epochs -->\n",
    "    <td>>...</td> <!-- train -->\n",
    "    <td>>...</td> <!--  test -->\n",
    "  </tr>\n",
    "\n",
    "  <tr> \n",
    "    <td>>...</td>   <!-- model -->\n",
    "    <td>>... </td> <!-- parameters -->\n",
    "    <td>>...</td> <!-- epochs -->\n",
    "    <td>>...</td> <!-- train -->\n",
    "    <td>>...</td> <!-- test -->\n",
    "  </tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Acer\n",
      " Volume Serial Number is 96D3-27E0\n",
      "\n",
      " Directory of C:\\Users\\Itamar\\Desktop\\DeepLearningFinalProject\\archive\\Kather_texture_2016_image_tiles_5000\\Kather_texture_2016_image_tiles_5000\n",
      "\n",
      "03/06/2021  11:05    <DIR>          .\n",
      "03/06/2021  11:05    <DIR>          ..\n",
      "06/06/2021  13:05    <DIR>          train\n",
      "03/06/2021  11:06    <DIR>          validation\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  365,183,729,664 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls archive\\Kather_texture_2016_image_tiles_5000\\Kather_texture_2016_image_tiles_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "healthy-height",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4504 images belonging to 8 classes.\n",
      "Found 496 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "pathTrain = 'archive\\\\Kather_texture_2016_image_tiles_5000\\\\Kather_texture_2016_image_tiles_5000\\\\train'\n",
    "pathTest= 'archive\\\\Kather_texture_2016_image_tiles_5000\\\\Kather_texture_2016_image_tiles_5000\\\\validation'\n",
    "my_batch_size = 32\n",
    "\n",
    "#Defining data generator WITH pre processing\n",
    "dataGen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "train_generator = dataGen.flow_from_directory(pathTrain,\n",
    "                                              batch_size=my_batch_size, \n",
    "                                              class_mode='categorical')\n",
    "\n",
    "dataGenTest = ImageDataGenerator()\n",
    "test_generator = dataGenTest.flow_from_directory(pathTest,                                              \n",
    "                                             batch_size=my_batch_size, \n",
    "                                             class_mode='categorical', \n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "similar-replica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 16)      2320      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 150, 150, 16)      2320      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 5, 5, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 639,240\n",
      "Trainable params: 639,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (150, 150, 3)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size = 3)) \n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')) \n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')) \n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size = 3)) \n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size = 3))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size = 3))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(8, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amended-stevens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/140 [==============================] - 347s 2s/step - loss: 1.7718 - accuracy: 0.3415 - val_loss: 1.5172 - val_accuracy: 0.5181\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 1\n",
    "history = model.fit(train_generator, steps_per_epoch=train_generator.samples/train_generator.batch_size, \n",
    "          validation_data=test_generator,\n",
    "          epochs=epochs, batch_size=20, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cultural-metabolism",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [16, 496]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3d1ba7d7fc4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mCM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# normalize each row (to sum = 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m--> 296\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16, 496]"
     ]
    }
   ],
   "source": [
    "yhat_1hot = model.predict(test_generator)\n",
    "yhat = np.argmax(yhat_1hot, axis=1)\n",
    "\n",
    "\n",
    "M = confusion_matrix(test_generator, yhat)\n",
    "\n",
    "CM = M / M.sum(axis=1) # normalize each row (to sum = 1)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "heatmap(CM,cmap='Blues',annot=True, fmt='.2%')\n",
    "plt.xlabel('Predicted label', fontsize=20)\n",
    "plt.ylabel('True label', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-graphics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-celtic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-listing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "refined-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'same',activation ='relu', input_shape = (150,150,3)))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'same',activation ='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same',activation ='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(256,activation = \"relu\"))          \n",
    "# model.add(Dense(64,activation = \"relu\"))\n",
    "# model.add(Dense(32,activation = \"relu\"))\n",
    "\n",
    "# model.add(Dense(9, activation = \"softmax\"))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fitting-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# epochs = 10\n",
    "# model.fit(train_generator, steps_per_epoch=train_generator.samples/train_generator.batch_size, \n",
    "#           validation_data=test_generator,\n",
    "#           epochs=epochs, batch_size=20, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
